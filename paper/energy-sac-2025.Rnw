\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{gensymb}

\setcopyright{acmcopyright}

\acmDOI{xx.xxx/xxx_x}

\acmISBN{979-8-4007-0629-5/25/03}

%Conference
\acmConference[SAC'25]{ACM SAC Conference}{March 31 –April 4, 2025}{Sicily, Italy}
\acmYear{2025}
\copyrightyear{2025}


\acmArticle{4}
\acmPrice{15.00}

\begin{document}
\title{Energy consumption of evolutionary algorithms written in low-level languages}
\subtitle{Comparison of C++ and {\sf zig} in key evolutionary algorithm functions}

\renewcommand{\shorttitle}{TBD}

\author{Ben Trovato}
\authornote{Boilerplate author names have been kept for double-blind review.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}
\email{trovato@corporation.com}

\author{G.K.M. Tobin}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}
\email{webmaster@marysville-ohio.com}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011074.10011075.10011079.10011080</concept_id>
       <concept_desc>Software and its engineering~Software design techniques</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10003809.10003716.10011136.10011797</concept_id>
       <concept_desc>Theory of computation~Optimization with randomized search heuristics</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10010061.10011795</concept_id>
       <concept_desc>Theory of computation~Random search heuristics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10003809.10003716.10011136.10011797.10011799</concept_id>
       <concept_desc>Theory of computation~Evolutionary algorithms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10010940.10010941.10010949.10010957.10010964</concept_id>
       <concept_desc>Software and its engineering~Power management</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Software and its engineering~Software design techniques}
\ccsdesc[300]{Theory of computation~Optimization with randomized search heuristics}
\ccsdesc[500]{Theory of computation~Random search heuristics}
\ccsdesc[500]{Theory of computation~Evolutionary algorithms}
\ccsdesc[500]{Software and its engineering~Power management}

\keywords{Green computing, software engineering, evolutionary algorithms, genetic algorithms, energy-aware algorithms}


\maketitle

\section{Introduction}

Software engineering aims to create products that work according to user desires; these desires include a series of technical requirements that have, until a short time ago, been synthesized in the word "efficiency," generally implying a high performance, that is, the capability to run a workload as fast as possible, or certainly without the user experimenting any kind of delay or lag. This is why the tools designed to create these applications or create executables that run directly on the operating system incorporate a series of {\em levers} that allow the user to make the program as fast as possible; processors and whole systems were also doubling in speed, in such a way that hardware and software combined to achieve faster and faster systems.

The second part of this increase in speed, which was generically called Moore's Law \cite{moore1965cramming}, however, is over, and has been declared dead \cite{zhang2022moore}. With the end of Moore's Law, another consequence is the reduction in the pace at which efficiency was increasing, in terms of the energy consumption of supercomputers, which until 2020 grew more or less exponentially. This is why contemporary hardware architectures put more emphasis on energy efficiency than on performance, and the software engineer needs to take this into account when designing software. However, unlike gains in performance which are mostly automatic, in the sense that a faster processor will automatically make any software faster, gains in energy efficiency from software engineering need a certain amount of manual tuning in the design, build and deployment of software, followed by an assessment of energy consumption that will reveal any possible gains.

While you can measure in very precise terms how many CPU instructions a specific high-level sentence generates, there are no sensors that work at the same level measuring energy consumption, with the only alternative being to measure system-wide consumption and develop a methodology that helps us measure how a specific workload spends energy. We will devote part of the paper to explaining the methodology we have been using, but that is not the main goal of the paper. Additionally, energy consumption is the product of power $x$ time, so certainly any change in performance will affect energy consumption. The issue here is that the other term of the product, power, needs not be constant, and it certainly is not in modern hardware architectures, which, as part of the energy efficiency improvements mentioned above, use dynamic power management techniques \cite{haj2018power,attia2017dynamic}, as well as asymmetric cores, with some of them being focused on performance and others on energy efficiency \cite{mittal2016survey}. Given an implementation of an algorithm, the operating system through hardware APIs will try to do its best to save energy by, for instance, dynamically changing processor frequency or turning on and off different parts of the core on demand. However, this will still have the constraint of running all and every machine code instruction that the compiler/interpreter needs, so there will still be huge differences between different implementations. And this is where our work starts, trying to find out where the lowest energy consumption is found among a panoply of different implementations of the same algorithm.

Our main goal is to advance in the development of a methodology that allows researchers in any field to find heuristic rules for how different decisions during the design and implementation of a scientific application can affect energy consumption. To do this, we need to focus first on what are the specific applications of the algorithms on which we concentrate our efforts and then, choose at what point of the decision-making process we will be working on.

Specifically, in the application domain, we are interested in evolutionary computation \cite{eiben2015introduction}, a family of population-based search and optimization algorithms that, in general, try to optimize a {\em fitness} function that scores how adequate (or {\em fit}) a candidate solution, represented through a suitable data structure, is, by creating a {\em population} of randomly generated such data structures, some of which are selected based on their fitness, with the rest thrown away. These data structures that represent the solutions are combined in pairs to create new solutions in an operation generally called {\em crossover}, to be then changed randomly, that is, {\em mutated}. This focus gives us the general context for energy profiling, but as indicated above, no existing tool can directly  {\em profile} the application to pinpoint specific {\em energetic} bottlenecks. Currently, what we have are performance profilers \cite{10.1145/183432.183527}, so if we want to isolate specific bottlenecks for energy measurements there are at least two paths of least resistance:

\begin{itemize}
\item To use a performance profiler to find the performance bottlenecks. As indicated, these need not be necessarily energy consumption bottlenecks, however if a real bottleneck is found, with a big difference over the next component in performance, we can assume it will also be consuming energy in the same order. In case of doubt, both components would need to be appraised energy-wise.
\item Use common heuristics to identify the functions that will be consuming the most energy, either by consuming it in every call, or by being called many times.
\end{itemize}

We will follow the latter approach in this paper, a choice that has been validated already by several papers in several areas of decision \cite{lion24-anon, icsoft-anon, icsoft23-anon, wivace23-anon}. Although there are other generic functions such as sorting, comparison, crossover, mutation, and a fitness function that will be called for every individual in the population, every generation. It is common that the count of fitness function calls goes up from tens of thousands to millions or even billions, so they are bound to take the bulk of the energy consumed by evolutionary algorithms implementations.

In several of these experiments, low-level languages have been proven to be faster and also less energy-consuming than VM based languages (like Java or Kotlin) or interpretered languages (like JavaScript or Python), which is more or less in accordance with works that use general-purpose workloads \cite{PEREIRA2021102609,pereira2017energy,gordillo2024programming}. However, \cite{lion24-anon} already revealed a slight mismatch between high-performance and low energy consumption. Besides, we will compare a language that has rarely entered a comparison list with the above mentioned languages, {\sf zig} \cite{friesen2023designing}, against the well-established C++. With its tight control on memory assignment, {\sf zig} leaves to the user a decision that is bound to have impact on energy consumption; besides, depending on results, it might be a good alternative to implement a low-energy consumption evolutionary computation library, if proved to be better in that sense than C++, already well established as a platform for evolutionary computation libraries \cite{friesen2023designing,jakobovic2024ecf,dreo2021paradiseo}.


Thus, in this paper, we will compare different implementations of the essential functions in evolutionary algorithms in two languages, C++ and {\sf zig}, looking at answering the following research questions:\begin{itemize}
\item Is there a combination of data structure and language that is significantly better than the rest?
\item Of the two factors that determine energy consumption, time and power, can we determine if there is a dominant term that makes the specific combination found above faster?
\item Does the energy-efficiency ranking of data structures hold across different languages, allowing us to conclude which data structures could be best for working with these problems in the context of evolutionary algorithms?
\end{itemize}

The rest of the paper is organized as follows: in the next section we will review the state of the art in energy consumption in programming languages, focusing specifically on evolutionary algorithms and other computational intelligence methodologies. The experimental methodology and setup will be described in Section \ref{sec:methodology}, along with the baseline results with some exploratory data analysis of same. We will finally draw conclusions and propose future work in Section \ref{sec:conclusions}.

\section{State of the art}

Until recently, the main focus of energy consumption in software engineering was mobile devices, where battery life is a primary concern \cite{ahmad2015review,alam2014energy}. However, with the increasing energy spending on IT \cite{koomey2007estimating}, including cloud computing, energy consumption has become a major concern in all areas of software engineering \cite{lee2024survey}. In the case of cloud computing, a survey by Katal et al. \cite{katal2023energy} analyzes the data center from a systems perspective and identifies new technologies and measurements at various levels. In the case of software, they consider four levels: the operating system, virtualization, data center, and application software. On the side of application development, the survey includes several techniques applied at a high level (for instance, application placement on a particular computer in a data center) to reduce energy usage in data-intensive and communication-intensive applications. In recent works, a more fine-grain analysis of the energy consumption of software has been carried out, focusing on the energy consumption of different programming languages and frameworks \cite{PEREIRA2021102609,gordillo2024programming}. Also, some works are focused on energy consumption measuring tools,  comparing the available options \cite{jay2023experimental}. Raffin and Trystram \cite{raffin2024dissecting} compared the efficiency of RAPL's various mechanisms for measuring energy consumption and their impact on parallel applications. Usually, developers are not trained or are aware of the energy consumption of their software \cite{pang2016what}, so this is an important step forward because these works will allow developers to make more informed decisions.

Regardless of language, there are works on other programming aspects that also
impact energy consumption. For instance, the flags used when compiling a
program. Kirkeby et al. \cite{kirkeby2024compiling} examined the impact on energy consumption of selectively
turning off 25 individual optimizations of the Glasgow Haskell Compiler when
running 18 benchmarks from the NoFib Haskell Benchmark Suite. They recorded
energy expenditure at three different initial temperatures (45 \degree C, 55
\degree C and 65 \degree C) and found that programs that started at 55 \degree
C tended to have lower energy consumption than those started at 45 \degree C or
65 \degree C.

On the side of computer science applications NLP has received special attention
because training deep learning (DL) networks on massive datasets is extremely
energy-demanding, and the carbon footprint of training large models is a
concern. Strubel et al. \cite{strubell2020energy} estimated that training one
model on GPUs can emit 626,155 pounds of carbon dioxide, roughly the lifetime
emissions of five cars (Car, avg incl. fuel, 1 lifetime 126,000 lbs).  In this
regard, researchers are comparing different libraries and frameworks for
training deep learning models. Another aspect that receives attention is the
selection of complied libraries with bindings to other languages. In a recent
work, Nahrsedt et al. \cite{nahrstedt2024empirical} compared the Rust-based
Polars library for data analysis against Pandas, a Python data manipulation
library. They found that the Rust-based Polars was the most energy efficient
for large dataframes (15 million and 60 million rows), spending about eight
times less energy on the synthetic DATs block and 67\% on the TPCH bencmarking
DATs block. There were no significant differences for smaller data frames. DL
frameworks are also compared in terms of energy consumption. For instance,
PyTorch runtime infrastructures are compared to TensorFlow´s, and the
infrastructure of the interchange format ONNX by Alizadeh and Castor
\cite{alizadeh2024green}, with the latter giving significant performance
improvements on most of cases.

There are still few works on the energy consumption of evolutionary algorithms.
Earlier works have focused on the programming languages and data structures
used to implement standard evolutionary operators \cite{merelo2017ranking-anon},
comparing high-level languages like Python and Java against low-level languages
like C and C++ \cite{lion24-anon} and emerging, performance-oriented
languages like {\sf zig} \cite{zig1-anon}. Cotta and Martínez \cite{10.1145/3638530.3664093}
explored the practical implication of running evolutionary algorithms several
times to obtain statistically significant results. Their work found that
introducing a delay between experiments can reduce energy consumption between
5\% and 8\%. This reduction is related to the increased CPU temperature from
running the last experiment at higher temperatures in the CPU, which might cause
the CPU to throttle or increase the energy consumption of fans.

\section{Methodology}\label{sec:methodology}

First and foremost, we need to decide the instrumentation we will use to measure energy. The instruments needed to measure them are roughly divided into hardware and software tools \cite{abdurachmanov2015techniques}, with the former being physical instruments that draw information from the power source or else sit between the main electricity source and the machine, thus accurately measuring the energy consumption of the machine. These are expensive scientific instruments, that besides have a synchronization issue to accurately reflect the start and end of the process we are measuring. The latter, however, are simply applications that run alongside the process under measurement, or run the process writing measurements when it ends, which has the disadvantage of introducing a small overhead, but on the other hand is perfectly synchronized. There are also many free, even free software options, so cost and precision have made us choose this option for our research.

These applications need to have some way of {\em reading} the sensors that the computer includes and that measure energy consumption in different devices. The sensors, and the API to read them, is processor and architecture specific; however, in Intel and AMD based processors a standard proposed by Intel called RAPL (Running Average Power Limit) \cite{rapl,david2010rapl} is available. This API provides a series of virtual registers that receive information on power consumed by different parts of the computer, usually called {\em domains}. We will especially pay attention to what is called the {\em package} (or PKG) domain, which includes the energy consumed by the whole processor package, all the cores, but also other so-called {\em uncore} components: the on-chip caches, for instance, but also in many cases the memory controller. This implies that although the energy consumed by  memory is not really considered here, memory operations will actually contribute to the measurements found here. This PKG is important for our purposes, not only because it might include the bulk of the energy consumption, but also because it is the minimum granularity level that is common to Intel and AMD processors. In general, PKG will include other sub-sensors in the case or Intel processors, but it will not do so in the case of AMD processors, so it is one of the measures that we can use to compare energy consumption in processors from different manufacturers \cite{khan2018rapl}.

In order to take actual measurements, there are several options; either link our programs to a library that taps the RAPL API, or use a command-line tool that runs our scripts and takes measurements when the process exits. Since we are working with different languages, not all of which have published libraries that work with RAPL \footnote{C++ certainly has one, but {\sf zig}, being a younger language, does not for the time being.}, we will opt for the latter.

And once again, we are faced with different options. Linux includes a command-line tool called {\sf perf} \cite{de2010new} that is concerned with all kind of performance measurements, including energy consumption. It is an excellent tool as long as the only type of systems you are going to measure use that operating system. In the past, however, we have used another tool called {\sf pinpoint} \cite{pinpoint}, a tool that is available for Linux as well as MacOS, and besides offers a single interface for different power consumption APIs. However the most important thing for this paper is that we have used it for the measurements in previous papers. Using the same tool again gives us the capability of making comparisons with the results published in those papers, since the methodology used to estimate consumption from RAPL register reading will be exactly the same. {\sf pinpoint} is free software released under the MIT license and can be downloaded from its repository at \url{https://github.com/osmhpi/pinpoint}. As we have done in other papers \cite{wivace23-anon,lion24-anon}, a Perl script launches the program a certain amount of times, set to 30 in this case, and works on averages.

For the experiments in this paper, we have used
a Linux machine {\tt 5.15.0-119-generic \#129~20.04.1-Ubuntu SMP} using AMD Ryzen 9 3950X 16-Core Processor, we use {\sf zig}  version 0.13.0, which is the last stable one at the time of writing this paper; {\tt g++} used is at version {\tt 10.5.0}. The {\sf pinpoint} tool has no versions, but we have used one compiled from source and commit hash {\tt dfee658}.

\subsection{Baseline measurements}

As indicated in the introduction, no energy profiling tool is able to disaggregate energy spent by a specific process, and {\sf pinpoint is no exception}; it measures energy spent by the devices under measurement for the duration of the process. If we really want to know what specific functions are spending, we have to make baseline measurements of a certain kind, and then run another application that includes the functions we are interested subtracting averages obtained in the first measurement.

There is no single way of establishing this measurement. In \cite{icsoft23-anon}, for instance, we simply took the average time spent by our programs and run a {\sf sleep} command for that average amount of time, measuring the energy consumed with essentially an empty program. This can certainly work in the general case, but in evolutionary algorithms there are two essential steps to apply any genetic operator or fitness function: the chromosomes need to be generated first, and then the operators applied to them. Generating chromosomes is a non-trivial operation, and how long it takes is related to the data structures that are used to store them. This can take a certain amount of time, which certainly can and should be separated from the application of the functions themselves. This is why, from \cite{wivace23-anon} on, we have started to use a different approach from the paragraph above, using a baseline program that generates chromosomes using the data structure we will use later. This program will follow the following Algorithm \ref{alg:chromgen}: \begin{algorithm}
\caption{Chromosome generation. $N$ is the total number of chromosomes generated, which for our paper is set to 40K.}
\label{alg:chromgen}
\begin{algorithmic}
\FOR{$i=1$ to $N$}
\STATE $c \leftarrow$ new chromosome
\STATE Append $c$ to $chromosomeArray$
\ENDFOR
\STATE Show the size of $chromosomeArray$
\end{algorithmic}
\end{algorithm}

Showing the size of the array of chromosomes simply disables the possibility of optimizing away all the generating code.

We will perform these baseline measurements for two different data structures, a character array (where every element is represented by a character 0 or 1) or a Boolean array (with every element represented by a value that is equal to {\sf True} or {\sf False}).

<<energy.sac.generation, echo=FALSE, message=FALSE, fig.cap="Energy vs. time for the chromosome generation problem (baseline). We use different shapes for data structures: circles for Boolean vectors, triangles for strings; languages are represented by different colors, with red representing C++ and blue zig; chromosome size is also represented by point size, with biggest corresponding to 2048, smallest to 512. Lines represent a linear regression over data form the same data structure.">>=
library(knitr)
zig.gen.bool <- read.csv("../data/sac-gen-bool-25-Sep-19-46-01.csv")
zig.gen.bool$data.structure <- "bvector"
zig.gen.string <- read.csv("../data/sac-gen-string-25-Sep-19-44-59.csv")
zig.gen.string$data.structure <- "string"
cpp.gen.bool <- read.csv("../data/c++-bvector-gen.csv")
cpp.gen.bool$data.structure <- "bvector"
cpp.gen.string <- read.csv("../data/cpp-string-27-Sep-20-36-43.csv")
cpp.gen.string$data.structure <- "string"

gen.data <- rbind(zig.gen.bool, zig.gen.string, cpp.gen.bool, cpp.gen.string)
gen.data$language <- c(rep("zig", nrow(zig.gen.bool)), rep("zig", nrow(zig.gen.string)), rep("c++", nrow(cpp.gen.bool)), rep("c++", nrow(cpp.gen.string)))
gen.data$point.size <- log2(gen.data$size)-7

library(dplyr)
gen.data %>% group_by(language, data.structure, size ) %>% summarise(mean(PKG), sd(PKG), mean(seconds), sd(seconds)) -> gen.data.summary

library(ggplot2)
ggplot(gen.data, aes(x=seconds, y=PKG, color=language)) + geom_point(aes(fill=data.structure,shape=data.structure,size=point.size)) + theme_minimal() + geom_smooth(method="lm", aes( group=interaction(language,data.structure))) + labs(x="Time (s)", y="Energy (J)") + theme(legend.position="none")
@

What we have represented in Figure \ref{fig:energy.sac.generation} is an energy consumption ($y$ axis) vs time ($x$ axis) plot for every experiment in the two languages, using the two target data structures. We have plotted a linear regression for the slope, that would represent the power consumed (being power equal to energy divided by time). We can appreciate in this picture how the differences across languages and data structures are not so clear cut. C++ seems (red) seems better for the smallest size (512 bits), but it is definitely worse for the largest size. Differences are small and nuanced enough to proceed without discarding any language or any data structure before analyzing the results obtained for the genetic operators, which we will do next.

<<energy.sac.generation.boxplot, echo=FALSE, message=FALSE, fig.cap="Energy boxplot for the chromosome generation problem (baseline).">>=
gen.data$size <- as.factor(gen.data$size)
ggplot(gen.data, aes(x=size, y=PKG, fill=language,linetype=data.structure)) + geom_boxplot() + theme_minimal() + labs(x="Data structure", y="Energy (J)")
@

Figure \ref{fig:energy.sac.generation.boxplot} synthesizes in a single chart the energy consumption for the chromosome generation problem for both languages and data structures. It grows much faster for C++ than for {\sf zig}. While it is slightly better for the smallest size, energy consumption for strings is virtually the same for the middle size, but it takes off for the largest size. It is curious to observe, however, that a specific data structure does not guarantee better energy consumption: Boolean vectors consume more energy in C++, while they consume {\em less} in {\sf zig}.

We need, however, to measure the consumption of actual genetic operators. We will do this in the next section.

\section{Results}\label{sec:results}

<<energy.sac.ops, echo=FALSE, message=FALSE, fig.cap="Delta energy from baseline vs. delta time from baseline for genetic operators problem. Shapes and colors as in the previous Figure.">>=
zig.ops.bool <- read.csv("../data/sac-2025-zig-ops-bool.csv")
zig.ops.bool$data.structure <- "bvector"
zig.ops.bool$delta.PKG <- zig.ops.bool$PKG - c(rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==512,]$`mean(PKG)`,30), rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==1024,]$`mean(PKG)`,30), rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==2048,]$`mean(PKG)`,30))
zig.ops.bool$delta.seconds <- zig.ops.bool$seconds - c(rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==512,]$`mean(seconds)`,30), rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==1024,]$`mean(seconds)`,30), rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==2048,]$`mean(seconds)`,30))
zig.ops.string <- read.csv("../data/sac-2025-zig-ops-string.csv")
zig.ops.string$data.structure <- "string"
zig.ops.string$delta.PKG <- zig.ops.string$PKG - c(rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="string" & gen.data.summary$size==512,]$`mean(PKG)`,30), rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="string" & gen.data.summary$size==1024,]$`mean(PKG)`,30), rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="string" & gen.data.summary$size==2048,]$`mean(PKG)`,30))
zig.ops.string$delta.seconds <- zig.ops.string$seconds - c(rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="string" & gen.data.summary$size==512,]$`mean(seconds)`,30), rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="string" & gen.data.summary$size==1024,]$`mean(seconds)`,30), rep(gen.data.summary[gen.data.summary$language=="zig" & gen.data.summary$data.structure=="string" & gen.data.summary$size==2048,]$`mean(seconds)`,30))
cpp.ops.bool <- read.csv("../data/cpp-bool-ops-30-Sep-09-03-07.csv")
cpp.ops.bool$data.structure <- "bvector"
cpp.ops.bool$delta.PKG <- cpp.ops.bool$PKG - c(rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==512,]$`mean(PKG)`,30), rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==1024,]$`mean(PKG)`,30), rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==2048,]$`mean(PKG)`,30))
cpp.ops.bool$delta.seconds <- cpp.ops.bool$seconds - c(rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==512,]$`mean(seconds)`,30), rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==1024,]$`mean(seconds)`,30), rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="bvector" & gen.data.summary$size==2048,]$`mean(seconds)`,30))
cpp.ops.string <- read.csv("../data/cpp-string-ops-30-Sep-09-00-11.csv")
cpp.ops.string$data.structure <- "string"
cpp.ops.string$delta.PKG <- cpp.ops.string$PKG - c(rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="string" & gen.data.summary$size==512,]$`mean(PKG)`,30), rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="string" & gen.data.summary$size==1024,]$`mean(PKG)`,30), rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="string" & gen.data.summary$size==2048,]$`mean(PKG)`,30))
cpp.ops.string$delta.seconds <- cpp.ops.string$seconds - c(rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="string" & gen.data.summary$size==512,]$`mean(seconds)`,30), rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="string" & gen.data.summary$size==1024,]$`mean(seconds)`,30), rep(gen.data.summary[gen.data.summary$language=="c++" & gen.data.summary$data.structure=="string" & gen.data.summary$size==2048,]$`mean(seconds)`,30))

ops.data <- rbind(zig.ops.bool, zig.ops.string, cpp.ops.bool, cpp.ops.string)
ops.data$language <- c(rep("zig", nrow(zig.gen.bool)), rep("zig", nrow(zig.gen.string)), rep("c++", nrow(cpp.gen.bool)), rep("c++", nrow(cpp.gen.string)))
ops.data$point.size <- log2(ops.data$size)-7

ops.data %>% group_by(language, data.structure, size) %>% summarise(mean(delta.PKG), sd(delta.PKG), mean(delta.seconds), sd(delta.seconds)) -> ops.data.summary

ggplot(ops.data, aes(x=delta.seconds, y=delta.PKG, color=language)) + geom_point(aes(fill=data.structure,shape=data.structure,size=point.size)) + theme_minimal() + labs(x="Time (s)", y="Energy (J)") + theme(legend.position="none")
@

We have performed a series of experiments using a triad of genetic operators: crossover, mutation and count-ones, following this algorithm
\begin{algorithm}
\caption{Appliation of genetic operators}
\label{alg:chromgen}
\begin{algorithmic}
\FOR{$i=1$ to $N$ by 2}
\STATE $c1 \leftarrow$ chromosome[i]
\STATE $c2 \leftarrow$ chromosome[i+1]
\STATE mutate($c1$)
\STATE mutate($c2$)
\STATE ($new\_c1, new\_c2$) $\leftarrow$ crossover($c1$, $c2$)
\STATE fitnessArray.append(count\_ones($new\_c1$)
\STATE fitnessArray.append(count\_ones($new\_c2$)
\STATE chromosomeArray.append($new\_c1$)
\STATE chromosomeArray.append($new\_c2$)
\ENDFOR
\STATE Show size of chromosomeArray
\end{algorithmic}
\end{algorithm}

The results of applying this chromosome has been shown in Figure~\ref{fig:enery.sac.ops}, where the average energy consumption and time for every size, language and data structure has been subtracted from the numeric value obtained. What we observe, however, is that applying the genetic operators {\em after} its generation results in less-than-0 energy consumption, as well as in some cases negative time. But what we see also is a clear separation between circles (corresponding to Boolean vectors) and triangles (corresponding to strings); the former {\em do} have some energy consumption, and can then be discarded as more energy-consuming, at least in the context of this experiment. However we do observe that the (negative) energy consumption is larger for C++ than for Zig, so even if it means combining two operations (generation and genetic operators) that are performed usually in disparate number of calls, we will compare the actual wallclock time. For the time being, we will not discuss the interpretation of this negative time/consumption, however, leaving that to the last Section.


\section{Conclusions}\label{sec:conclusions}
\begin{acks}
  Hidden for double-blind review
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{energy,GAs,ours,cplusplus}

\end{document}

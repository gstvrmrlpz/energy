\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables

\setcopyright{acmcopyright}

\acmDOI{xx.xxx/xxx_x}

\acmISBN{979-8-4007-0629-5/25/03}

%Conference
\acmConference[SAC'25]{ACM SAC Conference}{March 31 â€“April 4, 2025}{Sicily, Italy}
\acmYear{2025}
\copyrightyear{2025}


\acmArticle{4}
\acmPrice{15.00}

\begin{document}
\title{TBD}
\subtitle{}

\renewcommand{\shorttitle}{TBD}

\author{Ben Trovato}
\authornote{Boilerplate author names have been kept for double-blind review.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}
\email{trovato@corporation.com}

\author{G.K.M. Tobin}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}
\email{webmaster@marysville-ohio.com}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Green computing, software engineering, evolutionary algorithms, genetic algorithms, energy-aware algorithms}


\maketitle

\section{Introduction}

Software engineering aims to create products that work according to user desires; these desires include a series of technical requirements that have, until a short time ago, been synthesized in the word "efficiency," generally implying a high performance, that is, the capability to run a workload as fast as possible, or certainly without the user experimenting any kind of delay or lag. This is why the tools designed to create these applications or create executables that run directly on the operating system incorporate a series of {\em levers} that allow the user to make the program as fast as possible; processors and whole systems were also doubling in speed, in such a way that hardware and software combined to achieve faster and faster systems.

The second part of this increase in speed, which was generically called Moore's Law \cite{moore1965cramming}, however, is over, and has been declared dead \cite{zhang2022moore}. With the end of Moore's Law, another consequence is the reduction in the pace at which efficiency was increasing, in terms of the energy consumption of supercomputers, which until 2020 grew more or less exponentially. This is why contemporary hardware architectures put more emphasis on energy efficiency than on performance, and the software engineer needs to take this into account when designing software. However, unlike gains in performance which are mostly automatic, in the sense that a faster processor will automatically make any software faster, gains in energy efficiency from software engineering need a certain amount of manual tuning in the design, build and deployment of software, followed by an assessment of energy consumption that will reveal any possible gains.

While you can measure in very precise terms how many CPU instructions a specific high-level sentence generates, there are no sensors that work at the same level measuring energy consumption, with the only alternative being to measure system-wide consumption and develop a methodology that helps us measure how a specific workload spends energy. We will devote part of the paper to explaining the methodology we have been using, but that is not the main goal of the paper. Additionally, energy consumption is the product of power $x$ time, so certainly any change in performance will affect energy consumption. The issue here is that the other term of the product, power, needs not be constant, and it certainly is not in modern hardware architectures, which, as part of the energy efficiency improvements mentioned above, use dynamic power management techniques \cite{haj2018power,attia2017dynamic}, as well as asymmetric cores, with some of them being focused on performance and others on energy efficiency \cite{mittal2016survey}. Given an implementation of an algorithm, the operating system through hardware APIs will try to do its best to save energy by, for instance, dynamically changing processor frequency or turning on and off different parts of the core on demand. However, this will still have the constraint of running all and every machine code instruction that the compiler/interpreter needs, so there will still be huge differences between different implementations. And this is where our work starts, trying to find out where the lowest energy consumption is found among a panoply of different implementations of the same algorithm.

Our main goal is to advance in the development of a methodology that allows researchers in any field to find heuristic rules for how different decisions during the design and implementation of a scientific application can affect energy consumption. To do this, we need to focus first on what are the specific applications of the algorithms on which we concentrate our efforts and then, choose at what point of the decision-making process we will be working on.

Specifically, in the application domain, we are interested in evolutionary computation \cite{eiben2015introduction}, a family of population-based search and optimization algorithms that, in general, try to optimize a {\em fitness} function that scores how adequate (or {\em fit}) a candidate solution, represented through a suitable data structure, is, by creating a {\em population} of randomly generated such data structures, some of which are selected based on their fitness, with the rest thrown away. These data structures that represent the solutions are combined in pairs to create new solutions in an operation generally called {\em crossover}, to be then changed randomly, that is, {\em mutated}. This focus gives us the general context for energy profiling, but as indicated above, no existing tool can directly  {\em profile} the application to pinpoint specific {\em energetic} bottlenecks. Currently, what we have are performance profilers \cite{10.1145/183432.183527}, so if we want to isolate specific bottlenecks for energy measurements there are at least two paths of least resistance:

\begin{itemize}
\item To use a performance profiler to find the performance bottlenecks. As indicated, these need not be necessarily energy consumption bottlenecks, however if a real bottleneck is found, with a big difference over the next component in performance, we can assume it will also be consuming energy in the same order. In case of doubt, both components would need to be appraised energy-wise.
\item Use common heuristics to identify the functions that will be consuming the most energy, either by consuming it in every call, or by being called many times.
\end{itemize}

We will follow the latter approach in this paper, a choice that has been validated already by several papers in several areas of decision \cite{lion24-anon, icsoft-anon, icsoft23-anon, wivace23-anon}. Although there are other generic functions such as sorting, comparison, crossover, mutation, and a fitness function that will be called for every individual in the population, every generation. It is common that the count of fitness function calls goes up from tens of thousands to millions or even billions, so they are bound to take the bulk of the energy consumed by evolutionary algorithms implementations.

In several of these experiments, low-level languages have been proven to be faster and also less energy-consuming than VM based languages (like Java or Kotlin) or interpretered languages (like JavaScript or Python), which is more or less in accordance with works that use general-purpose workloads \cite{PEREIRA2021102609,pereira2017energy,gordillo2024programming}. However, \cite{lion24-anon} already revealed a slight mismatch between high-performance and low energy consumption. Besides, we will compare a language that has rarely entered a comparison list with the above mentioned languages, {\sf zig} \cite{friesen2023designing}, against the well-established C++. With its tight control on memory assignment, {\sf zig} leaves to the user a decision that is bound to have impact on energy consumption; besides, depending on results, it might be a good alternative to implement a low-energy consumption evolutionary computation library, if proved to be better in that sense than C++, already well established as a platform for evolutionary computation libraries \cite{friesen2023designing,jakobovic2024ecf,dreo2021paradiseo}.


Thus, in this paper, we will compare different implementations of the essential functions in evolutionary algorithms in two languages, C++ and {\sf zig}, looking at answering the following research questions:\begin{itemize}
\item Is there a combination of data structure and language that is significantly better than the rest?
\item Of the two factors that determine energy consumption, time and power, can we determine if there is a dominant term that makes the specific combination found above faster?
\item Does the energy-efficiency ranking of data structures hold across different languages, allowing us to conclude which data structures could be best for working with these problems in the context of evolutionary algorithms?
\end{itemize}

The rest of the paper is organized as follows: in the next section we will review the state of the art in energy consumption in programming languages, focusing specifically on evolutionary algorithms and other computational intelligence methodologies. The experimental methodology and setup will be described in Section \ref{sec:methodology}, along with the baseline results with some exploratory data analysis of same. % Something is missing here. - Mario

\section{State of the art}

Although there have been, since early in the century, different papers analyzing energy consumption in different programming languages \cite{PEREIRA2021102609,gordillo2024programming}, there are two issues with these: first, their results are shown on general workloads, and second, new or emerging languages like Zig are not included.

At any rate, these papers, especially the last one, establish the state of the art in several aspects: first, the experimental design and measurement methodology, and a general idea of how different languages in different categories spend energy and the differences in order of magnitude. Finally, results will be discussed and conclusions drawn in Section \ref{sec:conclusions}.

\section{Methodology}\label{sec:methodology}

First and foremost, we need to decide the instrumentation we will use to measure energy. The instruments needed to measure them are roughly divided into hardware and software tools \cite{abdurachmanov2015techniques}, with the former being physical instruments that draw information from the power source or else sit between the main electricity source and the machine, thus accurately measuring the energy consumption of the machine. These are expensive scientific instruments, that besides have a synchronization issue to accurately reflect the start and end of the process we are measuring. The latter, however, are simply applications that run alongside the process under measurement, or run the process writing measurements when it ends, which has the disadvantage of introducing a small overhead, but on the other hand is perfectly synchronized. There are also many free, even free software options, so cost and precision have made us choose this option for our research.

These applications need to have some way of {\em reading} the sensors that the computer includes and that measure energy consumption in different devices. The sensors, and the API to read them, is processor and architecture specific; however, in Intel and AMD based processors a standard proposed by Intel called RAPL (Running Average Power Limit) \cite{rapl,david2010rapl} is available. This API provides a series of virtual registers that receive information on power consumed by different parts of the computer, usually called {\em domains}. We will especially pay attention to what is called the {\em package} (or PKG) domain, which includes the energy consumed by the whole processor package, all the cores, but also other so-called {\em uncore} components: the on-chip caches, for instance, but also in many cases the memory controller. This implies that although the energy consumed by  memory is not really considered here, memory operations will actually contribute to the measurements found here. This PKG is important for our purposes, not only because it might include the bulk of the energy consumption, but also because it is the minimum granularity level that is common to Intel and AMD processors. In general, PKG will include other sub-sensors in the case or Intel processors, but it will not do so in the case of AMD processors, so it is one of the measures that we can use to compare energy consumption in processors from different manufacturers \cite{khan2018rapl}.

In order to take actual measurements, there are several options; either link our programs to a library that taps the RAPL API, or use a command-line tool that runs our scripts and takes measurements when the process exits. Since we are working with different languages, not all of which have published libraries that work with RAPL \footnote{C++ certainly has, but {\sf zig}, being a younger language, does not for the time being}, we will opt for the latter.

And once again, we are faced with different options. Linux includes a command-line tool called {\sf perf} \cite{de2010new} that is concerned with all kind of performance measurements, including energy consumption. It is an excellent tool as long as the only type of systems you are going to measure use that operating system. In the past, however, we have used another tool called {\sf pinpoint} \cite{pinpoint}, a tool that is available for Linux as well as MacOS, and besides offers a single interface for different power consumption APIs. However the most important thing for this paper is that we have used it for the measurements in previous papers. Using the same tool again gives us the capability of making comparisons with the results published in those papers, since the methodology used to estimate consumption from RAPL register reading will be exactly the same. {\sf pinpoint} is free software released under the MIT license and can be downloaded from its repository at \url{https://github.com/osmhpi/pinpoint}. As we have done in other papers \cite{wivace23-anon,lion24-anon}, a Perl script launches the program a certain amount of times, set to 30 in this case, and works on averages.


\subsection{Baseline measurements}

As indicated in the introduction, no energy profiling tool is able to disaggregate energy spent by a specific process, and {\sf pinpoint is no exception}; it measures energy spent by the devices under measurement for the duration of the process. If we really want to know what specific functions are spending, we have to make baseline measurements of a certain kind, and then run another application that includes the functions we are interested subtracting averages obtained in the first measurement.

There is no single way of establishing this measurement. In \cite{icsoft23-anon}, for instance, we simply took the average time spent by our programs and run a {\sf sleep} command for that average amount of time, measuring the energy consumed with essentially an empty program. This can certainly work in the general case, but in evolutionary algorithms there are two essential steps to apply any genetic operator or fitness function: the chromosomes need to be generated first, and then the operators applied to them. Generating chromosomes is a non-trivial operation, and how long it takes is related to the data structures that are used to store them. This can take a certain amount of time, which certainly can and should be separated from the application of the functions themselves. This is why, from \cite{wivace23-anon} on, we have started to use a different approach from the paragraph above, using a baseline program that generates chromosomes using the data structure we will use later.

\section{Results}\label{sec:results}

\section{Conclusions}\label{sec:conclusions}
\begin{acks}
  Hidden for double-blind review
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{energy,GAs,ours,cplusplus}

\end{document}

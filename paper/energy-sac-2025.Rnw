\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables

\setcopyright{acmcopyright}

\acmDOI{xx.xxx/xxx_x}

\acmISBN{979-8-4007-0629-5/25/03}

%Conference
\acmConference[SAC'25]{ACM SAC Conference}{March 31 â€“April 4, 2025}{Sicily, Italy}
\acmYear{2025}
\copyrightyear{2025}


\acmArticle{4}
\acmPrice{15.00}

\begin{document}
\title{TBD}
\subtitle{}

\renewcommand{\shorttitle}{TBD}

\author{Ben Trovato}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}
\email{trovato@corporation.com}

\author{G.K.M. Tobin}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}
\email{webmaster@marysville-ohio.com}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{ACM proceedings, \LaTeX, text tagging}


\maketitle

\section{Introduction}

Software engineering aims to create products that work according to user desires; these desires include a series of technical requirements that have until a short time ago been synthesized in the word "efficiency", generally implying high-performance, that is, the capability to run a workload as fast as possible, or certainly without the user experimenting any kind of delay or lag. This is why the tools designed to create these applications or create executables that run directly on the operating system are endowed with a series of {\em levers} that allow the user to make the program as fast as possible; processors and whole systems were also doubling in speed, in such a way that hardware and software combined to achieve faster and faster systems.

The second part of this increase in speed, which was generically called Moore's Law \cite{moore1965cramming}, however, is over, and has been declared dead \cite{zhang2022moore}. With it, another consequence of Moore's law: the increasing efficiency in terms of energy of supercomputers, which until 2020 grew more or less exponentially. So any increase in efficiency, for any sense of the term, needs to arrive in terms of software engineering. However, the emphasis on performance has left the software engineer with relatively few tools to measure or improve energy consumption. While you can measure in very precise terms how many CPU instructions a specific high-level sentence generates, there are no sensors that work at the same level measuring energy consumption, with the only alternative being to measure system-wide consumption and develop a methodology that help us measure how a specific workload spends energy. We will devote part of the paper to explain the methodology we have been using, but that is not the main goal of the paper.

Our main goal is to advance in the development of a methodology that allows researchers in any field to find heuristic rules for how different decisions during the design and implementation of a scientific application can affect energy consumption. In order to do this, we need first to focus on what are the specific applications on algorithms we are going to focus on, and then to fix at what point in the decision making process we will be working.

Specifically, we are interested on evolutionary computation \cite{eiben2015introduction}, a family of population-based search and optimization algorithms that, in general, try to optimize a {\em fitness} function that scores how adequate (or {\em fit}) a candidate solution, represented through a suitable data structure, is, by creating a {\em population} of randomly generated such data structures, some of which are selected based on their fitness, with the rest thrown away. These data structures that represent the solutions are combined in pairs to create new solutions in an operation generally called {\em crossover}, to then be changed randomly, that is, {\em mutated}. This focus gives us the general context for energy profiling, but as indicated above, there are no tools that are able to {\em profile} the application for specific {\em energetic} bottlenecks. What there is are performance profilers \cite{10.1145/183432.183527}, so if we want to spin off specific bottlenecks for energy measurements, there are at least two paths of least resistance:\begin{itemize}
\item To use a performance profiler to find the performance bottlenecks. As indicated, these need not be necessarily energy consumption bottlenecks, however if a real bottleneck is found, with a big difference over the next component in performance, we can assume it will also be consuming energy in the same order. In case of doubt, both components would need to be appraised energy-wise.
\item Use common heuristics to identify the functions that will be consuming the most energy, either by consuming it in every call, or by being called many times.
\end{itemize}

We will follow the latest approach in the paper. Although there are other generic functions such as sorting, or comparison, crossover, mutation and a fitness function will be called for every individual in the population, every generation, with common function call counts going up from tens of thousands to millions or even billions, so they are bound to take the bulk of the energy consumed by an implementation of evolutionary algorithms. This choice has been validated already by several papers in several areas \cite{lion24-anon, icsoft-anon, icsoft23-anon, wivace23-anon}.


\section{State of the art}

Although there have been, since early in the century, different papers analyzing energy consumption in different programming languages \cite{PEREIRA2021102609,gordillo2024programming}, there are two issues with these: first, their results are shown on general workloads, and second, new or emerging languages like Zig are not included.

At any rate, these papers, especially the last one, establish the state of the art in several aspects: first, the experimental design and measurement methodology, and a general idea of how different languages in different categories spend energy and the differences in order of magnitude.

\begin{acks}
  Hidden for double-blind review
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{energy,GAs,ours}

\end{document}
